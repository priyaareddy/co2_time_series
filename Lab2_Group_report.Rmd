---
title: "Global $CO_{2}$ Emissions 1995 and Pressent"
short: "What Keeling missed all these years"
journal: "AER" # AER, AEJ, PP, JEL
month: "`r format(Sys.Date(), '%m')`"
year: "`r format(Sys.Date(), '%Y')`"
vol: 0
issue: 0
keywords:
  - Replication
  - Modern Science
author:
  - name: Yuna Kim, Jinsoo Chung, Priya Reddy, Jocelyn Thai
acknowledgements: | 
  The authors would like to thank their instructors from MIDS 271 as well as acknowledge the foundational research and data collection work done by harles Keeling.
  
abstract: | 
  Global average temperatures have increased by more than 1â„ƒ from pre-industrial times to now, and it's common scientific belief that the increase of carbon emissions worldwide has played a critical role in this global warming. Scientists believe that one of the primary drivers of climate change is that continued, and increasing, human emissions of carbon dioxide and other greenhouse gases into the atmosphere. This rapid global warming can have and already has had significant impacts on global and local climates as well as natural and industrial systems across the world. Studying CO2 presense in the atmosphere is important in this context then in order to better understand not only one of the drivers of climate change, and in order to ascertain what steps we might be able to take to predict and manage the effects that will arise as a result of it.
  
header-includes: 
  - '\usepackage{graphicx}'
  - '\usepackage{booktabs}'
  
output: 'pdf_document'  
---

```{r load packages, echo = FALSE, message = FALSE, warning=FALSE}
#load all packages needed
library(tidyverse)
library(tsibble)
library(latex2exp)
library(lubridate)
library(patchwork)
library(magrittr)
library(tsibble)
library(feasts)
library(forecast)
library(stargazer)
library(sandwich)
library(lmtest)
library(fable)
library(MASS)

library(blsR)

library(httr)
library(jsonlite)

library(plyr)
library(dplyr)
library(tidyr)
library(fable)
library(gridExtra)


theme_set(theme_minimal())
knitr::opts_chunk$set(dpi=1000)
```


```{r setup, echo=FALSE}
## default to not show code, unless we ask for it.
knitr::opts_chunk$set(echo=FALSE)
options(digits = 3)
```


Understanding a changing climate, and what it means for the earth's inhabitants is of growing interest to the scientific and policy community. Although, at this point in 1997 it is not entirely clear what the consequences of this growing awareness will be, in this report we present likely outcomes under "business-as-usual" scenarios. In doing so, our hope, is to establish a series of possible futures, and, as evidence, technology, and policy develop over the coming decades, our goal is that we can theb weigh the impacts that carbon-emission reduction efforts might take. 

# Background 
## Carbon Emissions 
In this report we seek to understand if atmospheric CO2 levels have increased since 1997 and if we anticipate that this trend will continue. To do this we seek to answer 2 main questions:
> What is the best model we can use to model CO2 levels over time?
> Using this model, what do we forecast atmospheric CO2 levels will be? And how confident are we in these predictions?

High CO2 levels have been shown to negatively play into climate change, and the increase in atmospheric CO2 has cascading effects from global warming to ocean acidification. Due to these effects, we may notice food supply chain disruptions, natural disasters of increasing intensity, and habitat disruption of all ecosystems and the animal species living there. In an economical sense, understanding these patterns is important so we can prepare for any shifts and uncertainties in global supply chains that might arise for these new climate patterns. In total understanding the patterns in atmospheric CO2 concentrations is important both so that we can mitigate the existing effects of this increase and prevent continuing upwards trends in CO2. In addition if our report findings are significant, the findings could persuade other governmental organizations to increase their CO2 monitoring efforts and global preparedness efforts.

## Historical Trends in Atmospheric Carbon 

In 1958 Charles Keeling began continuous monitoring of atmospheric carbon dioxide concentrations from the Mauna Loa Observatory in Hawaii and soon observed a trend increase carbon dioxide levels in addition to the seasonal cycle. He was able to attribute this trend increase to growth in global rates of fossil fuel combustion. This trend has continued to the present, and is known as the "Keeling Curve,", this curve is plotted below. And we can see in this curve both an overall rising trend in CO2 levels as well as some sort of cyclic or seasonal pattern of atmospheric CO2 fluctuations each year.

```{r plot the keeling curve, echo = FALSE}
tsibble::as_tsibble(co2) %>%
  ggplot() + 
  aes(x=index, y=value) + 
  geom_line(color = 'steelblue') +
  labs(
    title = TeX(r'(Monthly Mean $CO_2$ 1959-1997)'),
    subtitle = 'The "Keeling Curve" constructed from CO2 observations from the Mauna Loa Observatory',
    x = 'Date (Month and Year)',
    y = TeX(r'($CO_2$ (parts per million))')
  )
```



# Models and Forecasts 
In this section, we evaluate two classes of models for answering our questions -- a linear time model and and an ARIMA model to assess which time series model is most appropriate to use to model CO2 levels over time. 

## Data EDA 
As a background on the data we are using it is important to note that this data measures the mean atmospheric CO2 concentration and was collected at the Mauna Loa Observatory in Hawaii. The CO2 levels in the data range from 313 parts per million by volume (ppmv) in March 1958 to 406 ppmv in November 2018. The data was also normalized to remove any influence from local contamination. Carbon dioxide measurements at the Mauna Loa Observatory in Hawaii are made with a type of infrared spectrophotometer, now known as a nondispersive infrared sensor, that is calibrated using World Meteorological Organization standards.


```{r,  create co2 ts, echo = FALSE}
co2_ts <- tsibble::as_tsibble(co2, index = index) 
colnames(co2_ts)[2] = "co2_val"

#add year and month term
#co2_ts['year'] = year(co2_ts$index)
#co2_ts['month'] = month(co2_ts$index)

#create log tsibble
log_ts <- co2_ts %>%
  mutate(log_co2 = log(co2_val))

#co2_ts

```


```{r, yearly co2 ts, echo = FALSE}
#create a yearly co2 tsibble
#where co2 levels are averaged over a whole year
co2_yr <- setNames(aggregate(co2_ts$co2_val, by=list(year(co2_ts$index)), mean), 
                      c("year", "yr_co2"))
#head(co2_yr)
```

```{r, co2 ts yearly trend plot, echo = FALSE}
#plot the yearly trend in co2
p6 <- co2_yr %>% 
  ggplot(aes(x = year, y = yr_co2)) + 
  geom_line() + 
  labs(title = 'CO2 Over Time Annual Trend',
     y = TeX(r'($CO_2$ parts per million)'))

p6
```

```{r EDA ,  echo = FALSE}
#inital EDA, look at initial plots as well as ACF, pacf, and distribution 
p1 <- co2_ts %>% 
  ggplot(aes(x = index, y = co2_val)) + 
  geom_line() + 
  labs(title = 'CO2 over Time')
p2 <- ggAcf(co2_ts$co2_val, type = 'correlation') + labs(title = 'ACF of CO2 Levels')
p3 <- ggAcf(co2_ts$co2_val, type = 'partial')+ labs(title = 'PACF of CO2 Levels')
p4 <- gg_lag(co2_val, data = co2_ts)
p5 <- co2_ts%>%
  ggplot(aes(x=co2_val, y= ..count..))+
  geom_histogram()

   (p1 | p2) /
   (p3 | p5)/
    (p4)+
   plot_annotation(
     title    = 'Title',
     subtitle = 'trend by month') & 
   labs(x = NULL, y = 'CO2') 
```

Observations:
- Time Series: From the CO2 Over Time plot, there is an obvious trend of increasing levels throughout time, and we can some regular oscillations which could mean there is seasonality in the data.
- ACF: There's a gradual decay in the ACF values over the lags, perhaps indicating there is a trend in the data. 
- PACF: PACF cuts off to zero after lag 2 and stays that way. However, at lags 12 and 13, the values become significant again. This suggests that although the model exhibits AR model-like behavior, there's some deviances in the data.
- Distribution: 

>Observations:
- Time Series: From the yearly CO2 Over Time plot, there is an obvious trend of increasing levels throughout time, but we can see that there is no seasonality now. This points to the period of the seasonal trend being a year.
- ACF: There's a gradual decay in the ACF values over the lags, indicating there is a still a trend in the data.

### Need to fix the EDA plots to better look at the trend, decompose seasonality, and to look at the trend og growth (acceleration)

## Linear Models 

### Time series decomposition 
```{r}
p11 <- co2_ts %>%
  ggplot(aes(x = index , y = co2_val ), colour = "gray") +
  geom_line(aes(y = co2_val), colour = "#D55E00") +
  labs(y = "CO2 emmision ", x="Date",
    title = "CO2 emmission")

p12 <- log_ts %>%
  ggplot(aes(x = index  , y = log_co2 )) +
  geom_line(aes(y= log_co2 ), colour = "#D55E00") +
  labs(y = " Log of CO2 emmision ", x="Date",
    title = "Log of CO2 emmission")

p11
p12
#grid.arrange(p11,p12, nrow = 1, ncol =2)
```


To fit linear time trend model to the `co2` series we will compare a regular time trend linear model to a quadratic time trend model. We will also fit a polynomial time trend model that incorporates seasonal dummy variables, and use this model to generate forecasts to the year 2020. Note that we will be evaluating using both the unscaled CO2 and the log(CO2) value for constructing our models.  

To begin our analysis, we fit a linear model of the form to our data: 

\begin{equation}
\label{eq:one}
\text{CO}_{2} = \phi_{0} + \phi_{1} + \epsilon_{eit}
\end{equation} 

```{r, echo= FALSE, warning=FALSE}
#Linear models (original scale)
lin_model <- co2_ts%>% model(trend_model = TSLM(co2_val ~ trend()))
quad_model <- co2_ts%>%  model(trend_model = TSLM(co2_val ~ trend() + I(trend()^2 )))

#linear models (Logged CO2 values)
log_lin_model <- log_ts%>%model(trend_model = TSLM(log_co2 ~ trend() ))
log_quad_model <- log_ts%>% model(trend_model = TSLM(log_co2 ~ trend() + I(trend()^2 )))

#Polynomial model with seasonal dummy (original scale) 
quadratic_season <- co2_ts %>%
  model(trend_model = TSLM(co2_val ~ trend()+I(trend()^2)+ season())) 

#Polynomial model with seasonal dummy (Logged CO2 values)
log_quadratic_season <- log_ts %>%
  model(trend_model = TSLM(log_co2 ~ trend()+I(trend()^2)+ season())) 

#stargazer(lin_model, quad_model,sea_poly_model, sea_poly_model_log, type = 'text', single.row = TRUE, 
 #         no.space = TRUE, 
  #        column.sep.width = "3pt", 
   #       font.size = "small")

```
```{r}
# Convert TSLM objects to tidy data frames
tidy1 <- tidy(lin_model)
tidy2 <- tidy(quad_model)
tidy3 <- tidy(quadratic_season)

# Combine tidy data frames into a single data frame
tidy_all <- rbind(tidy1, tidy2, tidy3)

# Create stargazer table
stargazer(tidy_all, type = "text", title = "My TSLM Models")
```

```{r plot models against data, echo=FALSE}
p37 <- augment(lin_model)%>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = co2_val, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = "Time",
       title = "linear model") 

p38<-augment(quad_model)%>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = co2_val, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = "Time",
       title = "quadratic model")

p39<-augment(log_lin_model)%>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = log_co2, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = "Time",
       title = "linear log model") 

p40<-augment(log_quad_model)%>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = log_co2, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = "Time",
       title = "log quadratic model") 

p41<-augment(quadratic_season)%>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = co2_val, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = "Time",
       title = "seasonal quadratic model") 

p42<-augment(log_quadratic_season)%>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = log_co2, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = "Time",
       title = "log seasonal quadratic model") 

grid.arrange(p37,p38,p39,p40,p41,p42, nrow = 3, ncol = 2)
```
>we can see that the log models appear to fit the data slightly better (some overshoot in the linear model around 1980s, whereas less in log linear model). Same with the quadratic models, appears like it's slightly overestimating in the quadratic model and is overestimating less (ie. in the middle of the seasonal variation) in the log quad model. zcan see this in the seasonal model too. Moving forward with working on the log(co2) data.


```{r, message=FALSE}
#plot model residuals 
#might need to delete

lin_resid <- log_lin_model %>% gg_tsresiduals()
lin_resid
quad_resid<- log_quad_model %>% gg_tsresiduals()
quad_resid
sea_resid<-log_quadratic_season %>% gg_tsresiduals()
sea_resid
#mod1_resi <- ggplot(aes(x=lin_model$fitted.values, y=lin_model$residuals), data = lin_model)+ geom_point()+geom_smooth(se=FALSE)+labs(title = "linear model")

#mod2_resi <- ggplot(aes(x=quad_model$fitted.values, y=quad_model$residuals), data = quad_model)+geom_point()+geom_smooth(se=FALSE)+labs(title = "Time quadratic model")

#mod3_resi <- ggplot(aes(x=lin_sea_model$fitted.values, y=lin_sea_model$residuals), data = lin_sea_model)+geom_point()+geom_smooth(se=FALSE)+labs(title = "linear model with Seasonal Dummy Variables")

#mod4_resi <- ggplot(aes(x=poly_model$fitted.values, y=poly_model$residuals), data = poly_model)+geom_point()+geom_smooth(se=FALSE)+labs(title = "Polynomial (3) model with Seasonal Dummy Variables")


#(mod1_resi + mod2_resi)/
# (mod3_resi + mod4_resi)

```


## Model Forecast


```{r predict 2020}
#generate predictions for up until 2020
# there are 23 years from Dec 1997 to Jan 2020 --> 276 months we need to predict for
predictions <- quadratic_season %>%
  fabletools::forecast(h=276)%>%
  autoplot(log_ts)

predictions
```


## ARIMA Models 

We will also generate a few ARIMA models and compare them to our linear model. To select our ARIMA model we use the AIC, the AIC selects against models with too many parameters. We know that there is a seasonal component to our data, so we will also select for seasonal AR and MA values and check if seasonal differencing will make the data stationary. I am choosing to use the ARIMA() function to select our data. This method will determine the optimal p,d,q,P,D,Q values as well as the optimal levels of differencing for us to have stationary data and will then select a model based on the metric we have chosen as our criteria (AIC). We can see interpret the resulting ARIMA model in terms of the number of (seasonal) AR terms, the level of differencing to acheive stationary data, and the (seasonalWe are including seasonality in our model because we can see in the decomposition as well as the time series plots that there is a strong possibility of a seasonal component in the CO2 levels. If adding a seasonal component improves the AIC score then such a model will be chosen.

```{r}
#takes a while, try to avoid re-running
#returns ARIMA(1,0,1)(4,1,0)[12] w/ drift
model.fit<-co2_ts %>%
  model(fable::ARIMA(co2_val ~ 1 + pdq(0:10,0:2,0:10) + PDQ(0:10,0:2,0:10), ic="aic", stepwise=F, greedy=F))

model.fit %>%
  report()

#model.fit %>%
#  augment() %>%
#  ACF(.resid) %>%
#  autoplot()+
#  labs(title = "ACF plot of ARIMA(1,0,1)(4,1,0)[12] model residuals")
```


With the AIC as our selection criteria we have estimated the model to be an ARIMA(1,0,1)(4,1,0)[12] with drift. This model has an AIC of 218, since this was the model that was chosen, this must be the smallest AIC value in the models that we have compared. This ARIMA(1,0,1)(4,1,0)[12] model can be interpreted by saying that there is 1 AR term, 1 MA term, 4 seasonal AR terms and the data had to be seasonally differenced once to be made stationary. the [12] at the end of the model also indicates


To be thorough we will also repeat the above process but instead using BIC and AICC as our selection metrics. In general the BIC is stricter than the AIC in penalizing additional parameters in our model, so it is possible that this selection process will result in a different model. When we repeated the above process using the bic or the aicc as our criteria, we found that we were sitll selecting the exact same ARIMA model, so we will move forwards with this model. 

```{r BIC ARIMA, echo=FALSE, eval=FALSE}
#DON'T re-run --> takes a long time and just gives same result as AIC
model.fit<-co2_ts %>%
  model(fable::ARIMA(co2_val ~ 1 + pdq(0:10,0:2,0:10) + PDQ(0:10,0:2,0:10), ic="bic", stepwise=F, greedy=F))

model.fit %>%
  report()

#model.fit2 %>%
 # augment() %>%
  #gg_tsdisplay(.resid, plot_type="partial") + labs( title = 'ARIMA(2,1,9)')
```

```{r AICC arima, echo=FALSE, eval=FALSE}
#DON'T re-run --> takes a long time and just gives same result as AIC
model.fit<-co2_ts %>%
  model(fable::ARIMA(co2_val ~ 1 + pdq(0:10,0:2,0:10) + PDQ(0:10,0:2,0:10), ic="aicc", stepwise=F, greedy=F))

model.fit %>%
  report()

#model.fit2 %>%
 # augment() %>%
  #gg_tsdisplay(.resid, plot_type="partial") + labs( title = 'ARIMA(2,1,9)')
```


## Forecasting Atmospheric CO2 Growth 

Now that we have a few different models that do a decent job at modeling our CO2 data, we want to use these models to generate predictions for us. 

### When will we reach 420 and 500 ppm CO2?
First we want to see when these models forecast that the atmospheric CO2 levels will reach 420ppm and 500ppm. Since we know there is a overall upwards trend as well as a seasonal component to the CO2 levels, when we generate these predictions we will actually want to look at both the first and the last times that the model predicts the atmospheric CO2 levels to be at these values. We know from our previous predictions for the linear model that even by 2020, the predicted CO2 ppm is not 420 ppm. Of course, there may be some variance in the predictions between the ARIMA and linear model but this does serve as a good guideline.

```{r}
predictions2 <- quadratic_season %>%
  fabletools::forecast(h=1000)

pred_420 <- predictions2 %>% 
  filter(
  predictions2[3][[1]] == 420
)

pred_500 <- predictions2 %>% filter(
  predictions2[4] == 500
)

pred_420
pred_500
```

### What do we predict CO2 levels will be in 2100 (80 years from now)?
We also want to look at the predictions for our models for the more distant future. Here we will look at what the model predicts will be the CO2 levels in the year 2100. We will also include the confidence intervals for these predictions.
```{r}
#forecast up until the end of 2100 and also calculate confidence interval
predictions3 <- quadratic_season %>%
  fabletools::forecast(h=1246)%>%
  hilo(level=c(90,95))%>%
  unpack_hilo("95%")

#select only the predictions for 2100
predictions3 <- predictions3[1225:1236,]
predictions3

```


# Report from the Point of View of the Present 

One of the very interesting features of Keeling and colleagues' research is that they were able to evaluate, and re-evaluate the data as new series of measurements were released. 

## Introduction 

In this introduction, you can assume that your reader will have **just** read your 1997 report. In this introduction, **very** briefly pose the question that you are evaluating, and describe what (if anything) has changed in the data generating process between 1997 and the present. 

## (3 points) Task 1b: Create a modern data pipeline for Mona Loa CO2 data.

The most current data is provided by the United States' National Oceanic and Atmospheric Administration, on a data page [[here](https://gml.noaa.gov/ccgg/trends/data.html)]. Gather the most recent weekly data from this page. (A group that is interested in even more data management might choose to work with the [hourly data](https://gml.noaa.gov/aftp/data/trace_gases/co2/in-situ/surface/mlo/co2_mlo_surface-insitu_1_ccgg_HourlyData.txt).) 

Create a data pipeline that starts by reading from the appropriate URL, and ends by saving an object called `co2_present` that is a suitable time series object. 

Conduct the same EDA on this data. Describe how the Keeling Curve evolved from 1997 to the present, noting where the series seems to be following similar trends to the series that you "evaluated in 1997" and where the series seems to be following different trends. This EDA can use the same, or very similar tools and views as you provided in your 1997 report. 

```{r, echo = FALSE }
co2_new <- read.csv("./co2_weekly_mlo.csv", header = TRUE, skip = 51)

co2_present<- co2_new %>%
  mutate(time_index = make_datetime(year, month, day)) %>%
  mutate(month_index = yearmonth(time_index)) %>%
  as_tibble(key = time_index) %>%
  filter(ndays != 0) %>%
  filter(year >= 1997)

head(co2_present)
```


```{r,  echo = FALSE }
timeplot <- co2_present %>%
  ggplot() + 
  aes(x=time_index, y=average) + 
  geom_line(color = 'steelblue') +
  labs(
    title = TeX(r'(Mean $CO_2$ from 1997 to Present)'),
    subtitle = 'weekly data',
    x = 'Date',
    y = TeX(r'($CO_2$ parts per million)')
  )

hist <- co2_present %>% 
  ggplot()+
  geom_histogram(aes(x= average), binwidth = 0.5) + 
  labs( title = 'Histogram of CO2 levels', x = 'CO2')

acf <- ggAcf(co2_present$average, type = 'correlation') + labs(title = 'ACF of CO2 Levels')
pacf <-ggAcf(co2_present$average, type = 'partial') + labs(title = 'PACF of CO2 Levels')

(timeplot + hist) / (acf + pacf)
```





## (1 point) Task 2b: Compare linear model forecasts against realized CO2

Descriptively compare realized atmospheric CO2 levels to those predicted by your forecast from a linear time model in 1997 (i.e. "Task 2a"). (You do not need to run any formal tests for this task.) 
```{r}

```



## (1 point) Task 3b: Compare ARIMA models forecasts against realized CO2  

Descriptively compare realized atmospheric CO2 levels to those predicted by your forecast from the ARIMA model that you fitted in 1997 (i.e. "Task 3a"). Describe how the Keeling Curve evolved from 1997 to the present. 



## (3 points) Task 4b: Evaluate the performance of 1997 linear and ARIMA models 

In 1997 you made predictions about the first time that CO2 would cross 420 ppm. How close were your models to the truth? 

After reflecting on your performance on this threshold-prediction task, continue to use the weekly data to generate a month-average series from 1997 to the present, and compare the overall forecasting performance of your models from Parts 2a and 3b over the entire period. (You should conduct formal tests for this task.) 
```{r}
#make data month-average series from 1997 to the present:
co2_present_bymonth <- setNames(aggregate(co2_present$average, by=list(co2_present$month_index), mean), 
                      c("by_month", "tot_CO2"))

#head(co2_present_bymonth)
co2_present_bymonth %>%
  as_tsibble(index=by_month) %>%
  gg_tsdisplay(tot_CO2, plot_type="partial") + xlab( "Date") + ylab( "US Total Cases")
```



## (4 points) Task 5b: Train best models on present data

Seasonally adjust the weekly NOAA data, and split both seasonally-adjusted (SA) and non-seasonally-adjusted (NSA) series into training and test sets, using the last two years of observations as the test sets. For both SA and NSA series, fit ARIMA models using all appropriate steps. Measure and discuss how your models perform in-sample and (psuedo-) out-of-sample, comparing candidate models and explaining your choice. In addition, fit a polynomial time-trend model to the seasonally-adjusted series and compare its performance to that of your ARIMA model.



## (3 points) Task Part 6b: How bad could it get?

With the non-seasonally adjusted data series, generate predictions for when atmospheric CO2 is expected to be at 420 ppm and 500 ppm levels for the first and final times (consider prediction intervals as well as point estimates in your answer). Generate a prediction for atmospheric CO2 levels in the year 2122. How confident are you that these will be accurate predictions?


# Conclusions 

What to conclude is unclear. 


While the most plausible model that we estimate is reported in the main, "Modeling" section, in this appendix to the article we examine alternative models. Here, our intent is to provide a skeptic that does not accept our assessment of this model as an ARIMA of order (1,2,3) an understanding of model forecasts under alternative scenarios. 


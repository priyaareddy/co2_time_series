---
title: "test - Global $CO_{2}$ Emissions 1995 and Pressent"
short: "What Keeling missed all these years"
journal: "AER" # AER, AEJ, PP, JEL
month: "`r format(Sys.Date(), '%m')`"
year: "`r format(Sys.Date(), '%Y')`"
vol: 0
issue: 0
keywords:
  - Replication
  - Modern Science
author:
  - name: Yuna Kim, Jinsoo Chung, Priya Reddy, Jocelyn Thai
acknowledgements: | 
  The authors would like to thank their instructors from MIDS 271 as well as acknowledge the foundational research and data collection work done by Charles Keeling.
  
abstract: | 
  Global average temperatures have increased by more than 1℃ from pre-industrial times to now, and it's common scientific belief that the increase of carbon emissions worldwide has played a critical role in this global warming. Scientists believe that one of the primary drivers of climate change is that continued, and increasing, human emissions of carbon dioxide and other greenhouse gases into the atmosphere. This rapid global warming can have and already has had significant impacts on global and local climates as well as natural and industrial systems across the world. Studying CO2 presense in the atmosphere is important in this context then in order to better understand not only one of the drivers of climate change, and in order to ascertain what steps we might be able to take to predict and manage the effects that will arise as a result of it.
  
header-includes: 
  - '\usepackage{graphicx}'
  - '\usepackage{booktabs}'
  
output: 'pdf_document' 
margin: 1cm
---

```{r load packages, echo = FALSE, message = FALSE, warning=FALSE}
#load all packages needed
library(tidyverse)
library(tsibble)
library(latex2exp)
library(lubridate)
library(patchwork)
library(magrittr)
library(tsibble)
library(feasts)
library(forecast)
library(stargazer)
library(sandwich)
library(lmtest)
library(fable)
library(MASS)

library(blsR)

library(httr)
library(jsonlite)

library(plyr)
library(dplyr)
library(tidyr)
library(fable)
library(gridExtra)


theme_set(theme_minimal())
knitr::opts_chunk$set(dpi=1000)
```


```{r setup, echo=FALSE}
## default to not show code, unless we ask for it.
knitr::opts_chunk$set(echo=FALSE)
##default to display all plots centered unless we adjust them
knitr::opts_chunk$set(fig.align='center')
##default to display 5 digits of every number
options(digits = 5)
```


Understanding a changing climate, and what it means for the earth's inhabitants is of growing interest to the scientific and policy community. Although, at this point in 1997 it is not entirely clear what the consequences of this growing awareness will be, in this report we present likely outcomes under "business-as-usual" scenarios. In doing so, our hope, is to establish a series of possible futures, and, as evidence, technology, and policy develop over the coming decades, our goal is that we can theb weigh the impacts that carbon-emission reduction efforts might take. In this report we seek to understand if atmospheric CO2 levels have increased since 1997 and if we anticipate that this trend will continue. To do this we seek to answer 2 main questions:
> What is the best model we can use to model CO2 levels over time?
> Using this model, what do we forecast atmospheric CO2 levels will be? And how confident are we in these predictions?



# Background 
### Carbon Emissions 

High CO2 levels have been shown to negatively play into climate change, and the increase in atmospheric CO2 has cascading effects from global warming to ocean acidification. Due to these effects, we may notice food supply chain disruptions, natural disasters of increasing intensity, and habitat disruption of all ecosystems and the animal species living there. In an economical sense, understanding these patterns is important so we can prepare for any shifts and uncertainties in global supply chains that might arise for these new climate patterns. In total understanding the patterns in atmospheric CO2 concentrations is important both so that we can mitigate the existing effects of this increase and prevent continuing upwards trends in CO2. In addition if our report findings are significant, the findings could persuade other governmental organizations to increase their CO2 monitoring efforts and global preparedness efforts.

### Measuring Atmospheric Carbion 
As a background on the data we are using it is important to note that this data measures the mean atmospheric CO2 concentration and was collected at the Mauna Loa Observatory in Hawaii. Mauna Loa Observatory was chosen by Keeling to record the observations since it was far from other continents and had sparse vegetation --  thereby mitigating some of the location influence on the data. The data was also normalized to remove any influence from local contamination. Carbon dioxide measurements at the Mauna Loa Observatory in Hawaii are made with a type of infrared spectrophotometer, now known as a nondispersive infrared sensor, that is calibrated using World Meteorological Organization standards [^Anal. Chem. 2010, 82, 19, 7865–7870. Publication Date:June 11, 2010. https://doi.org/10.1021/ac1001492]. 

### Historical Trends in Atmospheric Carbon 

In 1958 Charles Keeling began continuous monitoring of atmospheric carbon dioxide concentrations from the Mauna Loa Observatory in Hawaii and soon observed a trend increase carbon dioxide levels in addition to the seasonal cycle. He was able to attribute this trend increase to growth in global rates of fossil fuel combustion. This trend has continued to the present, and is known as the "Keeling Curve," and is plotted below. At first glance we can see in this curve both an overall upwards trend in CO2 levels as the mean seems to be increasing over time. We can also see some sort of cyclic or seasonal pattern of atmospheric CO2 fluctuations each year.

```{r plot the keeling curve, echo = FALSE,  fig.width=5.25, fig.height=3.5}
tsibble::as_tsibble(co2) %>%
  ggplot() + 
  aes(x=index, y=value) + 
  geom_line(color = 'steelblue') +
  labs(
    title = TeX(r'(Monthly Mean $CO_2$ 1959-1997)'),
    subtitle = 'The "Keeling Curve" constructed from CO2 observations from the Mauna Loa Observatory',
    x = 'Date (Month and Year)',
    y = TeX(r'($CO_2$ (parts per million))')
  )
```


```{r,  create co2 ts, echo = FALSE}
#load in the data as tsibble
co2_ts <- tsibble::as_tsibble(co2, index = index) 
colnames(co2_ts)[2] = "co2_val"

#create logged co2 tsibble
log_ts <- co2_ts %>%
  mutate(log_co2 = log(co2_val))
#co2_ts
```


```{r, yearly co2 ts, echo = FALSE}
#create a yearly co2 tsibble
#where co2 levels are averaged over a whole year
co2_yr <- setNames(aggregate(co2_ts$co2_val, by=list(year(co2_ts$index)), mean), 
                      c("year", "yr_co2"))
#head(co2_yr)
```

Looking at the data there are three areas of interest that we may want to look into further -- the upwards trend, the seasonality, and the irregularities we observe in the data. 

To begin with we can look at the trend in our data. To better visualize the trend we can plot the yearly average CO2 level (thereby eliminating the seasonality we observed in our initial plot) over time. 

```{r, co2 ts yearly trend plot, echo = FALSE,  fig.width=5.25, fig.height=3.5}
#plot the yearly trend in co2
p6 <- co2_yr %>% 
  ggplot(aes(x = year, y = yr_co2)) + 
  geom_line() + 
  labs(title = TeX(r'(Figure 1. $CO_2$ Level Over Time (Annual Average))'),
     y = TeX(r'($CO_2$ (ppm))'),
     x = "Year")

p6
```
We can see in Figure 1, that when we aggregate by year, there is a clearly year-over-year upwards trend in the $CO_2$ levels in the data. The lack of seasonality in this graph also indicates that our previous assumption that the seasonality was yearly is correct. 

This seasonality can be better visualized by plotting CO2 levels for each year that we have observations in our dataset.
```{r seasonality plot, fig.width=5.25, fig.height=3.5}
p2 <-co2_ts %>%
  gg_season(co2_val, labels = "right") +
  labs(y = "CO2 (ppm)", x = "Month",
       title = "Figure 2. Seasonal plot: Monthly CO2 levels (ppm)")
p2
```

Figure 2 shows a clear seasonal pattern in our data. It appears as though CO2 levels peak at around May and then dip to a minimum in September and October. To further explore our initial observations on seasonality we can look at our ACF, PACF, and Lag plots.

```{r EDA ,  echo = FALSE, message=FALSE}
#inital EDA, look at  ACF, pacf, distribution, lag

p3 <- ggAcf(co2_ts$co2_val, lag.max = 48, type = 'correlation') + 
  labs(title = 'Fig 3A. ACF of Monthly CO2')

p4 <- ggAcf(co2_ts$co2_val, type = 'partial', lag.max = 48)+ 
  labs(title = 'Fig 3B.PACF of Monthly CO2')

p5 <- co2_ts%>%
  ggplot(aes(x=co2_val, y= ..count..))+
  geom_histogram()

p6 <- co2_ts %>%
  gg_lag(co2_val, geom = "point", lag=1:12) +
  labs(title = "Fig 3C. CO2 Lagged scatterplots",
       y = "CO2 (ppm)",x = "lag(Total, k)")+
  theme(axis.text.x = element_text(size = 6,angle = 45, hjust = 1))


(p3/p4) |p6

```

In Figure 3C, the lag plot, there is a very strong positive correlation across all lags, reflecting the seasonality of our data.This is also reflected in the ACF plot, Figure 3A, which decays gradually indicating a trend in our data, annd also has a subtle rise in correlation at around every 12th lag. This behavior in the ACF plot indicates a seasonal or cyclic pattern in our data. The Figure 3B PACF cuts off to zero after lag 2 and stays that way. However, at lags 12 and 13, the values become significant again. This suggests that although the model exhibits AR model-like behavior, there's some deviances in the data.

In looking at the patterns in our data we are also interested in assessing whether the upwards trend we observed is accelerating -- that is we want to look at the trend of the growth rate of CO2 levels. At a high level, knowing this could help researchers assess the efficacy of global efforts to curb CO2 emissions.
### Time series decomposition 
```{r}

# decomposition

```

```{r}
p11 <- co2_ts %>%
  ggplot(aes(x = index , y = co2_val ), colour = "gray") +
  geom_line(aes(y = co2_val), colour = "#D55E00") +
  labs(y = "CO2 emmision ", x="Date",
    title = "CO2 emmission")

p12 <- log_ts %>%
  ggplot(aes(x = index  , y = log_co2 )) +
  geom_line(aes(y= log_co2 ), colour = "#D55E00") +
  labs(y = " Log of CO2 emmision ", x="Date",
    title = "Log of CO2 emmission")

p11
p12
#grid.arrange(p11,p12, nrow = 1, ncol =2)
```


# Models and Forecasts 
In this section, we evaluate two classes of models for answering our questions -- a linear time model and and an ARIMA model to assess which time series model is most appropriate to use to model CO2 levels over time. 

## Linear Models 

To fit linear time trend model to the `co2` series we will compare a regular time trend linear model to a quadratic time trend model. We will also fit a polynomial time trend model that incorporates seasonal dummy variables, and use this model to generate forecasts to the year 2020. Note that we will be evaluating using both the unscaled CO2 and the log(CO2) value for constructing our models.  

To begin our analysis, we fit a linear model of the form to our data: 

\begin{equation}
\label{eq:one}
\text{CO}_{2} = \phi_{0} + \phi_{1} + \epsilon_{eit}
\end{equation} 

```{r, echo= FALSE, warning=FALSE}
#Linear models (original scale)
lin_model <- co2_ts%>% model(trend_model = TSLM(co2_val ~ trend()))
quad_model <- co2_ts%>%  model(trend_model = TSLM(co2_val ~ trend() + I(trend()^2 )))

#linear models (Logged CO2 values)
log_lin_model <- log_ts%>%model(trend_model = TSLM(log_co2 ~ trend() ))
log_quad_model <- log_ts%>% model(trend_model = TSLM(log_co2 ~ trend() + I(trend()^2 )))

#Polynomial model with seasonal dummy (original scale) 
quadratic_season <- co2_ts %>%
  model(trend_model = TSLM(co2_val ~ trend()+I(trend()^2)+ season())) 

#Polynomial model with seasonal dummy (Logged CO2 values)
log_quadratic_season <- log_ts %>%
  model(trend_model = TSLM(log_co2 ~ trend()+I(trend()^2)+ season())) 
```


```{r plot models against data, echo=FALSE}
p37 <- augment(lin_model)%>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = co2_val, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = "Time",
       title = "linear model") 

p38<-augment(quad_model)%>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = co2_val, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = "Time",
       title = "quadratic model")

p39<-augment(log_lin_model)%>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = log_co2, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = "Time",
       title = "linear log model") 

p40<-augment(log_quad_model)%>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = log_co2, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = "Time",
       title = "log quadratic model") 

p41<-augment(quadratic_season)%>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = co2_val, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = "Time",
       title = "seasonal quadratic model") 

p42<-augment(log_quadratic_season)%>%
  ggplot(aes(x = index)) +
  geom_line(aes(y = log_co2, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = "Time",
       title = "log seasonal quadratic model") 

grid.arrange(p37,p38,p39,p40,p41,p42, nrow = 3, ncol = 2)
```
We can see that the log models appear to fit the data slightly better (some overshoot in the linear model around 1980s, whereas less in log linear model). Same with the quadratic models, appears like it's slightly overestimating in the quadratic model and is overestimating less (ie. in the middle of the seasonal variation) in the log quad model. zcan see this in the seasonal model too. Moving forward with working on the log(co2) data.


```{r, message=FALSE, echo=FALSE}
#plot model residuals 
#might need to delete

lin_resid <- log_lin_model %>% gg_tsresiduals()
lin_resid
quad_resid<- log_quad_model %>% gg_tsresiduals()
quad_resid
sea_resid<-log_quadratic_season %>% gg_tsresiduals()
sea_resid


#mod1_resi <- ggplot(aes(x=lin_model$fitted.values, y=lin_model$residuals), data = lin_model)+ geom_point()+geom_smooth(se=FALSE)+labs(title = "linear model")

#mod2_resi <- ggplot(aes(x=quad_model$fitted.values, y=quad_model$residuals), data = quad_model)+geom_point()+geom_smooth(se=FALSE)+labs(title = "Time quadratic model")

#mod3_resi <- ggplot(aes(x=lin_sea_model$fitted.values, y=lin_sea_model$residuals), data = lin_sea_model)+geom_point()+geom_smooth(se=FALSE)+labs(title = "linear model with Seasonal Dummy Variables")

#mod4_resi <- ggplot(aes(x=poly_model$fitted.values, y=poly_model$residuals), data = poly_model)+geom_point()+geom_smooth(se=FALSE)+labs(title = "Polynomial (3) model with Seasonal Dummy Variables")


#(mod1_resi + mod2_resi)/
# (mod3_resi + mod4_resi)

```


## Model Forecast


```{r predict 2020}
#generate predictions for up until 2020
# there are 23 years from Dec 1997 to Jan 2020 --> 276 months we need to predict for
predictions <- quadratic_season %>%
  fabletools::forecast(h=276)%>%
  autoplot(co2_ts)

predictions
```


## ARIMA Models 

We will also generate a few ARIMA models and compare them to our linear model. To select our ARIMA model we use the AIC, the AIC selects against models with too many parameters. We know that there is a seasonal component to our data, so we will also select for seasonal AR and MA values and check if seasonal differencing will make the data stationary. I am choosing to use the ARIMA() function to select our data. This method will determine the optimal p,d,q,P,D,Q values as well as the optimal levels of differencing for us to have stationary data and will then select a model based on the metric we have chosen as our criteria (AIC). We can see interpret the resulting ARIMA model in terms of the number of (seasonal) AR terms, the level of differencing to acheive stationary data, and the (seasonalWe are including seasonality in our model because we can see in the decomposition as well as the time series plots that there is a strong possibility of a seasonal component in the CO2 levels. If adding a seasonal component improves the AIC score then such a model will be chosen.

```{r}
#takes a while, try to avoid re-running
#returns ARIMA(1,0,1)(4,1,0)[12] w/ drift

#including the constant because our data does not have a mean of 0 (even if we were to remove the trend)
model.fit<-co2_ts %>%
  model(fable::ARIMA(co2_val ~ 1 + pdq(0:10,0:2,0:10) + PDQ(0:10,0:2,0:10), ic="aic", stepwise=F, greedy=F))

model.fit %>%
  report()

#model.fit %>%
#  augment() %>%
#  ACF(.resid) %>%
#  autoplot()+
#  labs(title = "ACF plot of ARIMA(1,0,1)(4,1,0)[12] model residuals")
```


With the AIC as our selection criteria we have estimated the model to be an ARIMA(1,0,1)(4,1,0)[12] with drift. This model has an AIC of 218, since this was the model that was chosen, this must be the smallest AIC value in the models that we have compared. This ARIMA(1,0,1)(4,1,0)[12] model can be interpreted by saying that there is 1 AR term, 1 MA term, 4 seasonal AR terms and the data had to be seasonally differenced once to be made stationary. the [12] at the end of the model also indicates


To be thorough we will also repeat the above process but instead using BIC and AICC as our selection metrics. In general the BIC is stricter than the AIC in penalizing additional parameters in our model, so it is possible that this selection process will result in a different model. When we repeated the above process using the bic or the aicc as our criteria, we found that we were sitll selecting the exact same ARIMA model, so we will move forwards with this model. 

```{r BIC ARIMA, echo=FALSE, eval=FALSE}
#DON'T re-run --> takes a long time and just gives same result as AIC
#model.fit<-co2_ts %>%
#  model(fable::ARIMA(co2_val ~ 1 + pdq(0:10,0:2,0:10) + PDQ(0:10,0:2,0:10), ic="bic", stepwise=F, greedy=F))

#model.fit %>%
#  report()
```

```{r AICC arima, echo=FALSE, eval=FALSE}
#DON'T re-run --> takes a long time and just gives same result as AIC
#model.fit<-co2_ts %>%
#  model(fable::ARIMA(co2_val ~ 1 + pdq(0:10,0:2,0:10) + PDQ(0:10,0:2,0:10), ic="aicc", stepwise=F, greedy=F))

#model.fit %>%
#  report()
```


## Forecasting Atmospheric CO2 Growth 

Now that we have a few different models that do a decent job at modeling our CO2 data, we want to use these models to generate predictions for us. 

### When will we reach 420 and 500 ppm CO2?
First we want to see when these models forecast that the atmospheric CO2 levels will reach 420ppm and 500ppm. Since we know there is a overall upwards trend as well as a seasonal component to the CO2 levels, when we generate these predictions we will actually want to look at both the first and the last times that the model predicts the atmospheric CO2 levels to be at these values. We know from our previous predictions for the linear model that even by 2020, the predicted CO2 ppm is not 420 ppm. Of course, there may be some variance in the predictions between the ARIMA and linear model but this does serve as a good guideline. 

When we do this we can then filter our predictions to find the times when the CO2 levels are predicted to be 420 and 500ppm. We should note that these are approximations as the true predictions from our model are not integers and instead are the mean of a prediction distribution. As a result we had to round these prediction values to find where the predicted CO2 will reach these levels. 

Our ARIMA model predicts that the first time that the atmospheric CO2 levels will to be 420ppm is April 2038, and the last time is October 2042. To be more accurate, we found that the predicted CO2 values will reach 420ppm for the first time sometime in between March 2038 and April 2038 and for the last time sometime in between September 2042 and October 2042.  We can also see that the first time that the atmospheric CO2 levels are predicted to be 500ppm is March 2101, and the last time is October 2105 Again, to be more accurate, we found that the predicted CO2 values will reach 500ppm for the first time sometime in between March 2101 and April 2101 and the for the last time sometime in between October 2105 and November 2105. 

```{r fig.width=5.25, fig.height=3.5}
predictions2 <- model.fit %>%
  fabletools::forecast(h=1400)%>%
  hilo(level=95)%>%
  unpack_hilo("95%")


colnames(predictions2)[4] = "point_prediction"

#Filter for 420ppm with some tolerance since these are doubles
ppm420 <- predictions2 %>% 
  as.data.frame()%>%
  filter((point_prediction <= 422) & (point_prediction >= 417) )
#ppm420[2:4]

#Filter for 500ppm with some tolerance since these are doubles
ppm500 <- predictions2 %>% 
  as.data.frame()%>%
  filter((point_prediction <= 503) & (point_prediction >= 497) )
#ppm500[2:4]

colnames(predictions2)[5] = "ci_lower"
colnames(predictions2)[6] = "ci_upper"


predictions2%>%
  autoplot(.vars = point_prediction)+
  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = 0.1)+
  geom_hline(aes(yintercept = 420, color = "420ppm"), linetype = "dashed")+
  geom_hline(aes(yintercept = 500, color = "500ppm"), linetype = "dashed")+
  labs(title = "Predictions for Co2 levels (1998-2113)", 
       color = " ")+
  ylab("Date")+
  xlab("Prediction CO2 Level (ppm)")
```

### What do we predict CO2 levels will be in 2100 (103 years from now)?
We also want to look at the predictions for our models for the more distant future. Here we will look at what the model predicts will be the CO2 levels in the year 2100. We will also include the confidence intervals for these predictions.
```{r}
#forecast up until the end of 2100 and also calculate confidence interval
predictions3 <- model.fit %>%
  fabletools::forecast(h=1236)%>%
  hilo(level=95)%>%
  unpack_hilo("95%")

colnames(predictions3)[4] = "point_prediction"
colnames(predictions3)[5] = "ci_lower"
colnames(predictions3)[6] = "ci_upper"


#select only the predictions for 2100
predictions3 <- predictions3[1225:1236,2:6]


predictions3%>%
  autoplot(.vars = point_prediction)+
  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = 0.1, linetype="dashed",color="red")+
  labs(title = "Predictions for Co2 levels in 2100", 
       color = " ")+
  ylab("Date")+
  xlab("Prediction CO2 Level (ppm)")

```

# Report from the Point of View of the Present 

One of the very interesting features of Keeling and colleagues' research is that they were able to evaluate, and re-evaluate the data as new series of measurements were released. 

## Introduction 

In this introduction, you can assume that your reader will have **just** read your 1997 report. In this introduction, **very** briefly pose the question that you are evaluating, and describe what (if anything) has changed in the data generating process between 1997 and the present. 

## (3 points) Task 1b: Create a modern data pipeline for Mona Loa CO2 data.

The most current data is provided by the United States' National Oceanic and Atmospheric Administration, on a data page [[here](https://gml.noaa.gov/ccgg/trends/data.html)]. Gather the most recent weekly data from this page. (A group that is interested in even more data management might choose to work with the [hourly data](https://gml.noaa.gov/aftp/data/trace_gases/co2/in-situ/surface/mlo/co2_mlo_surface-insitu_1_ccgg_HourlyData.txt).) 

Create a data pipeline that starts by reading from the appropriate URL, and ends by saving an object called `co2_present` that is a suitable time series object. 

Conduct the same EDA on this data. Describe how the Keeling Curve evolved from 1997 to the present, noting where the series seems to be following similar trends to the series that you "evaluated in 1997" and where the series seems to be following different trends. This EDA can use the same, or very similar tools and views as you provided in your 1997 report. 

```{r, echo = FALSE }
co2_new <- read.csv("./co2_weekly_mlo.csv", header = TRUE, skip = 51)
co2_new <- co2_new[1:6]

co2_present<- co2_new %>%
  mutate(time_index = as.Date(make_datetime(year, month, day))) %>%
  mutate(month_index = yearmonth(time_index)) %>%
  tsibble::as_tsibble(index = month_index, key = time_index) %>%
  filter(ndays != 0) %>%
  filter(year > 1997) %>%
  index_by(date = month_index)%>%
  dplyr::summarise(co2_avg = mean(average))

colnames(co2_present)[1] = "time_index"
colnames(co2_present)[2] = "average"

tail(co2_present)
```


```{r,  echo = FALSE }
timeplot <- co2_present %>%
  ggplot() + 
  aes(x=time_index, y=average) + 
  geom_line(color = 'steelblue') +
  labs(
    title = TeX(r'(Mean $CO_2$ from 1997 to Present)'),
    subtitle = 'weekly data',
    x = 'Date',
    y = TeX(r'($CO_2$ parts per million)')
  )

hist <- co2_present %>% 
  ggplot()+
  geom_histogram(aes(x= average), binwidth = 0.5) + 
  labs( title = 'Histogram of CO2 levels', x = 'CO2')

acf <- ggAcf(co2_present$average, type = 'correlation') + labs(title = 'ACF of CO2 Levels')
pacf <-ggAcf(co2_present$average, type = 'partial') + labs(title = 'PACF of CO2 Levels')

(timeplot + hist) / (acf + pacf)
```

## (1 point) Task 2b: Compare linear model forecasts against realized CO2

Descriptively compare realized atmospheric CO2 levels to those predicted by your forecast from a linear time model in 1997 (i.e. "Task 2a"). (You do not need to run any formal tests for this task.) 
```{r}
prediction_lin <- quadratic_season %>%
  fabletools::forecast(h=302)

p_linear <- ggplot() +
  geom_line(aes(x = prediction_lin$index, y = prediction_lin$.mean, colour = "Quadratic Seasonal Model")) +
  geom_line(aes(x = co2_present$time_index, y = co2_present$average, colour = 'Actual')) + 
  labs(x = "Date", y = 'CO2 levels (ppm)',
       title = "Quadratic Seasonal Model Predictions vs Actual Data for 1998-2023", 
       color = '') 


p_linear
```


The linear model is a fairly close match to the actual values, it doe seem like from 200-2005 it has a tendency to slightly overestimate the CO2 levels, and then from 2015 to 2020 it appears to slightly underestimate the CO2 levels. 


## (1 point) Task 3b: Compare ARIMA models forecasts against realized CO2  

Descriptively compare realized atmospheric CO2 levels to those predicted by your forecast from the ARIMA model that you fitted in 1997 (i.e. "Task 3a"). Describe how the Keeling Curve evolved from 1997 to the present. 

```{r}
prediction_arima <- model.fit %>%
  fabletools::forecast(h=302)

p_arima <- ggplot() +
  geom_line(aes(x = prediction_arima$index, y = prediction_arima$.mean, colour = "Arima_Model")) +
  geom_line(aes(x = co2_present$time_index, y = co2_present$average, colour = 'Actual')) + 
  labs(x = "Time", y = 'CO2 levels (ppm)',
       title = "ARIMA Model Predictions vs Actual Data for 1998-2023", 
       color = '') 


p_arima
```

The ARIMA model  deviates from the actual values. The model values are similar until 2000, though the model does tend to underestimate the CO2 levels even during this time period. The model really deviates from the actual values at around 2003, as it starts to severely underestimate the CO2 levels. It appears that the keeling curve has shown some acceleration in the increase of the mean over time, and this might explain the increasing gap between the ARIMA model performance and the true CO2 levels as we move away from 1997.


## (3 points) Task 4b: Evaluate the performance of 1997 linear and ARIMA models 

In 1997 you made predictions about the first time that CO2 would cross 420 ppm. How close were your models to the truth? 

In 1997 we predicted that the CO2 levels would cross 420ppm at around April 2038, but from the actual data we can see that the CO2 levels cross 420ppm by February 2022, which is an about a 15 year difference. This is in-line with what we observed in our ARIMA model predictions as the  ARIMA model was underestimating the CO2 levels in comparison to the actual CO2 levels.
```{r}
#Filter for 420ppm with some tolerance since these are doubles
# cross 420 ppm by 2022 Feb
ppm420 <- co2_present %>% 
  as.data.frame()%>%
  filter((average <= 422) & (average >= 419) )
#ppm420

```


We can also assess the overall performance of our model using a test metric such as the RMSE. The RMSE is the root mean squared error and is a metric that captures on average how far our predictions are from the actual values. It is calculated with the formula $\sqrt{\sum_{i=1}^{N}\frac{(y_{i}-\hat{y}_i)}{N}}$ where N is the number of observations, $y_i$ is the ith actual CO2 value and $\hat{y}_i$ is the ith CO2 prediction -- in essence we are squaring the sum of the differences between the actual and predicted values, and then dividing by the number of observations before taking the square root. When we calculate the RMSE for our two models of interest we can see that the ARIMA model performs worse than the quadratic seasonal model by an order of roughly 10. The Quadratic Seasonal model's RMSE is 0.79235, meaning that on average the Quadratic Seasonal model prediction is off by about 0.79235 ppm. By contrast the ARIMA model RMSE is 8.6259 meaning that on average the ARIMA model is off by acout 8.6259 ppm. 

```{r}
# Use RMSE as a formal test to evaluate the models 

# Calculate the RMSE
rmse_lin <- sqrt(mean((prediction_lin$.mean - co2_present$average)^2))
rmse_arima <- sqrt(mean((prediction_arima$.mean - co2_present$average)^2))

# Print the result
print(cat("Quadratic Seasonal Model RMSE =", rmse_lin))
print(cat("ARIMA model RMSE =", rmse_arima))
```


## (4 points) Task 5b: Train best models on present data

Seasonally adjust the weekly NOAA data, and split both seasonally-adjusted (SA) and non-seasonally-adjusted (NSA) series into training and test sets, using the last two years of observations as the test sets. For both SA and NSA series, fit ARIMA models using all appropriate steps. Measure and discuss how your models perform in-sample and (psuedo-) out-of-sample, comparing candidate models and explaining your choice. In addition, fit a polynomial time-trend model to the seasonally-adjusted series and compare its performance to that of your ARIMA model.


```{r, warnings = FALSE}
# Seasonally adjust the data
co2_present_sa <- setNames(aggregate(co2_present$average, by=list(year(co2_present$time_index)), mean), 
                      c("year", "yr_co2"))
co2_present_nsa <- co2_present
  
  #setNames(aggregate(co2_present$average, by=list(co2_present$time_index), mean), 
                     # c("by_month", "tot_CO2"))


#Split into training and test
co2_train_sa <- co2_present_sa %>% filter(year <= max(year)-2) %>% as_tsibble(index = year)
co2_test_sa <- co2_present_sa %>% filter(year > max(year)-2)  %>% as_tsibble(index = year)


co2_train_nsa <- co2_present_nsa %>% 
  as_tsibble(index=time_index) %>% filter(year(time_index) <= max(year(time_index))-2) 
co2_test_nsa <- co2_present_nsa %>% 
  as_tsibble(index=time_index) %>% filter(year(time_index) > max(year(time_index))-2) 

#Fit the seasonally adjusted model
sa.model.fit <-co2_train_sa %>%
  model(fable::ARIMA(yr_co2 ~ 1 + pdq(0:10,0:2,0:10) + PDQ(0:10,0:2,0:10), ic="aic", stepwise=F, greedy=F))

sa.model.graph <- augment(sa.model.fit)%>%
  ggplot(aes(x = year)) +
  geom_line(aes(y = yr_co2, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = "Time",
       title = "Seasonally Adjusted Model") 


#Fit the non-seasonally adjusted model
nsa.model.fit <-co2_train_nsa %>%
  model(fable::ARIMA(average ~ 1 + pdq(0:10,0:2,0:10) + PDQ(0:10,0:2,0:10), ic="aic", stepwise=F, greedy=F))

nsa.model.graph <- augment(nsa.model.fit)%>%
  ggplot(aes(x = time_index)) +
  geom_line(aes(y = average, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = "Time",
       title = "Non-Seasonally Adjusted Model") 


sa.model.graph |  nsa.model.graph

#model.fit %>%
#  report()

#model.fit %>%
#  augment() %>%
#  ACF(.resid) %>%
#  autoplot()+
#  labs(title = "ACF plot of ARIMA(1,0,1)(4,1,0)[12] model residuals")
```

```{r}
#Seasonally adjusted model predictions
sa_prediction <- sa.model.fit %>%
  fabletools::forecast(h=2, level = 0.95) %>%
  autoplot() +
  geom_line(aes(x = co2_test_sa$year, y = co2_test_sa$yr_co2, colour = 'Actual')) + 
  labs(x = "Time", y = 'CO2 levels',
       title = "Seasonally Adjusted Model (ARIMA) Prediction vs Actual") 

#Non-seasonally adjusted model predictions
nsa_prediction <- nsa.model.fit %>%
  fabletools::forecast(h=14, level = 0.95) %>%
  autoplot() +
  geom_line(aes(x = co2_test_nsa$time_index, y = co2_test_nsa$average, colour = 'Actual')) + 
  labs(x = "Time", y = 'CO2 levels',
       title = "Non Seasonally Adjusted Model (ARIMA) Prediction vs Actual") 

sa_prediction / nsa_prediction


```

```{r}
#Seasonally adjusted model predictions
sa_prediction <- sa.model.fit %>%
  fabletools::forecast(h=2, level = 0.95) %>%
  autoplot() +
  geom_line(aes(x = co2_test_sa$year, y = co2_test_sa$yr_co2, colour = 'Actual')) + 
  labs(x = "Time", y = 'CO2 levels',
       title = "Seasonally Adjusted Model (ARIMA) Prediction vs Actual") 

#Non-seasonally adjusted model predictions
quad_sa <- co2_train_sa%>%  model(trend_model = TSLM(yr_co2 ~ trend() + I(trend()^2 )))

quad_predictions <- quad_sa %>%
  fabletools::forecast(h=2)%>%
  autoplot(co2_test_sa) + labs(x = "Time", y = 'CO2 levels',
       title = "Seasonally Adjusted Model (Quad) Prediction vs Actual") 

sa_prediction | quad_predictions


```

Both seasonally adjusted and non-seasonally adjust models perform well. The data fits perfectly within the training dataset. with forecasts, the seasonally adjust model falls short, deviating from the actual model. The non-seasonally adjusted ARIMA model predicts close to what the actual values were.The quadratic model performs the worst compared to the ARIMA model when using Seasonally Adjusted models.

## (3 points) Task Part 6b: How bad could it get?

With the non-seasonally adjusted data series, generate predictions for when atmospheric CO2 is expected to be at 420 ppm and 500 ppm levels for the first and final times (consider prediction intervals as well as point estimates in your answer). Generate a prediction for atmospheric CO2 levels in the year 2122. How confident are you that these will be accurate predictions?

```{r}
co2_present_prediction <- nsa.model.fit %>%
  fabletools::forecast(h=1212)
  
co2_present_pred_graph <- co2_present_prediction %>%  
  autoplot() + geom_hline(aes(yintercept = 420, color = "420ppm"), linetype = "dashed")+
  geom_hline(aes(yintercept = 500, color = "500ppm"), linetype = "dashed")+
  labs(title = "Predictions for Co2 levels 1998-2122", 
       color = " ")+
  ylab("Date")+
  xlab("Prediction CO2 Level (ppm)")

#display 5 digits (in this case 2 decimal points)
options(digits = 5)

#Filter for 420ppm with some tolerance since these are doubles
ppm420 <- co2_present_prediction %>% 
  as.data.frame()%>%
  filter((.mean <= 422) & (.mean >= 419) )
#ppm420

#Filter for 500ppm with some tolerance since these are doubles
ppm500 <- co2_present_prediction %>% 
  as.data.frame()%>%
  filter((.mean <= 503) & (.mean >= 497) )
#ppm500

co2_present_pred_graph
```

```{r}

co2_2122 <- co2_present_prediction %>% filter(year(time_index) == 2122)
mean(co2_2122$.mean)

```
It's predicted that the average co2 level in 2122 would be 637.42 ppm. There's a possibility that the actual values are worse than predicted judging from the predictions in the Keeling curve. If there's no mitigation in the co2 levels in the future, the co2 levels won't likely decline.

# Conclusions 

What to conclude is unclear. 


While the most plausible model that we estimate is reported in the main, "Modeling" section, in this appendix to the article we examine alternative models. Here, our intent is to provide a skeptic that does not accept our assessment of this model as an ARIMA of order (1,2,3) an understanding of model forecasts under alternative scenarios. 

